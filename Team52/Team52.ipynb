{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import re, string, math\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split(x, y, train_size=0.8, shuffle=False):\n",
    "    return train_test_split(x, y, train_size=train_size, test_size=1-train_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count_map(text):\n",
    "    word_regex = re.compile(r\"[\\w']+\")\n",
    "    words = word_regex.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute cosine similarity between vectorized tokens using sklearn\n",
    "def compute_cosine_similarity(x, vectorizer):\n",
    "    transformed_x = vectorizer.fit_transform(x)\n",
    "    return cosine_similarity(transformed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings\n",
    "def compute_cosine_similarity2(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection]) #dot product of the intersection set\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_retweet(tweet):\n",
    "    if not(re.compile(r'RT @')).search(tweet):\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_tweet(tweet):\n",
    "    if not(len(tweet) < 20):\n",
    "        return tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab_set = set(words.words())\n",
    "def filter_nonenglish_tweet(tweet):\n",
    "    tweet_count_vec = create_word_count_map(tweet)\n",
    "    tweet_word_set = set(tweet_count_vec.keys())\n",
    "    #sum of all values of count vec\n",
    "    tweet_wordcount = sum(tweet_count_vec[w] for w in tweet_count_vec.keys() ) \n",
    "    \n",
    "    non_english_set = tweet_word_set - english_vocab_set\n",
    "    non_english_wordcount = sum(tweet_count_vec[fw] for fw in non_english_set) \n",
    "   \n",
    "    if not( (100 - (non_english_wordcount/tweet_wordcount)*100) < 15):\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_elem(df, columns, data_arr, idx=0, ignore_index=True):\n",
    "    df = df.append(pd.DataFrame(data=[data_arr[idx]], columns = columns), \n",
    "                                     ignore_index=ignore_index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_similar_tweets(df, vectorizer): #df is primarily filtered       \n",
    "    x = df['text']\n",
    "    cos_matrix = compute_cosine_similarity(x, vectorizer)\n",
    "    rows = cos_matrix.shape[0]\n",
    "    rep_pairs = [[]] #debugging\n",
    "    repeated = []\n",
    "\n",
    "    #no need to iterate over the entire array\n",
    "    for r in range(0, rows-2): \n",
    "        for c in range(r+1, rows-1):\n",
    "            if cos_matrix[r][c]>=0.9:\n",
    "                rep_pairs.append([r,c])\n",
    "                if c in repeated:\n",
    "                    continue\n",
    "                else: \n",
    "                    if r in repeated:\n",
    "                        repeated.append(c)\n",
    "                    else:\n",
    "                        repeated.append(r)\n",
    "    #for n similar tweets, only n-1 are added to the repeated; no priorities                \n",
    "    return df.drop(repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_similar_tweets2(df): #df is primarily filtered\n",
    "    tweets = df['text']\n",
    "    df_columns = df.columns\n",
    "    df_filtered = pd.DataFrame(columns = df_columns)\n",
    "    df_data = np.array(df)\n",
    "    df_filtered = append_df_elem(df_filtered, df_columns, df_data, 0)\n",
    "    \n",
    "    vec1 = Counter()\n",
    "    vec2 = [Counter()]\n",
    "    vec2[0] = create_word_count_map(np.array(df_filtered['text'])[0])\n",
    "    \n",
    "    for idx, ft in enumerate(tweets):\n",
    "        vec1 = create_word_count_map(ft)\n",
    "        for idx2, fft in enumerate(df_filtered):\n",
    "            if compute_cosine_similarity2(vec1, vec2[idx2])>=0.9:\n",
    "                break\n",
    "            else:\n",
    "                df_filtered = append_df_elem(df_filtered, df_columns, df_data, idx)\n",
    "                vec2.append(vec1)\n",
    "                \n",
    "    return df_filtered #final data frame      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df, vectorizer):\n",
    "    tweets = df['text']\n",
    "    df_columns = df.columns\n",
    "    df_filtered = pd.DataFrame(columns = df_columns)\n",
    "    df_data = np.array(df)\n",
    "    \n",
    "    for idx, tweet in enumerate(tweets):\n",
    "        if not(filter_short_tweet(tweet) == None) and not(filter_retweet(tweet) == None) and not(filter_nonenglish_tweet(tweet) == None):\n",
    "            df_filtered = append_df_elem(df_filtered, df_columns, df_data, idx)     \n",
    "    \n",
    "    #df_filtered = filter_similar_tweets2(df_filtered) \n",
    "    df_filtered = filter_similar_tweets(df, vectorizer)\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lancaster_stemmer = LancasterStemmer()\n",
    "def preprocess_and_tokenize(tweet):\n",
    "    tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) \n",
    "    regex_word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words += list(string.punctuation)\n",
    "    stop_words += ['``', \"''\", \"'s\"]\n",
    "    \n",
    "    tokens = set()\n",
    "    \n",
    "    t = tweet\n",
    "    t = t.casefold() \n",
    "    #remove urls\n",
    "    t = re.sub(r'https?:\\/\\/\\S+\\s*', '', t)\n",
    "    t = re.sub(r'www\\.\\S+\\s*', '', t) \n",
    "    #remove numbers, remove numbers followed by letters\n",
    "    t = re.sub(r'[0-9](\\S*)', '', t)\n",
    "    #tweet_tokens; a list\n",
    "    t_tokens = tweet_tokenizer.tokenize(t)\n",
    "    t = ' '.join(t_tokens)\n",
    "\n",
    "    t_tokens = regex_word_tokenizer.tokenize(t) #tokenize tweet with regex\n",
    "    t_tokens = [tt for tt in t_tokens if not tt in stop_words] #remove stop words & punctuation\n",
    "    t_tokens = [wnl.lemmatize(tt) for tt in t_tokens] #lemmatize all tokens\n",
    "    t_tokens = [porter_stemmer.stem(tt) for tt in t_tokens] #stem all tokens\n",
    "\n",
    "    for tt in t_tokens:    \n",
    "        tokens.add(tt) #tt: one tweet token, in t_tokens of a tweet\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis: Twitter_Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads ds \n",
    "df = pd.read_csv('~/.kaggle/twitter-airline-sentiment/Tweets.csv')\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data set\n",
    "x = np.array(df['text'])\n",
    "y = np.array(df['airline_sentiment'])\n",
    "x_train, x_test, y_train, y_test = df_split(x, y, train_size=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11712, 7460)\n"
     ]
    }
   ],
   "source": [
    "#Count Vectorizer; constructs a document-word count matrix\n",
    "cv = CountVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "transformed_train_cv = cv.fit_transform(x_train)\n",
    "print(transformed_train_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11712, 7460)\n"
     ]
    }
   ],
   "source": [
    "#Tf-Idf Vectorizer; constructs a tf-idf matrix\n",
    "tidv = TfidfVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "transformed_train_tfidf = tidv.fit_transform(x_train)\n",
    "print(transformed_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the test inputs using cv and tfidf(no fitting is done on test)\n",
    "transformed_test_cv = cv.transform(x_test)\n",
    "\n",
    "transformed_test_tfidf = tidv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Multi-nomial NB classifier:\n",
    "#using count vectorizer: feed the classifier train inputs, and labels\n",
    "mnb_classifier1 = MultinomialNB(alpha=1)\n",
    "mnb_classifier1.fit(transformed_train_cv, y_train)\n",
    "#using tfidf vectorizer\n",
    "mnb_classifier2 = MultinomialNB(alpha=1)\n",
    "mnb_classifier2.fit(transformed_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for MNB classifier with CountVectorizer 75.1707650273224\n",
      "f1_score for MNB classifier with Tf-Idf Vectorizer 68.4084699453552\n"
     ]
    }
   ],
   "source": [
    "#use the trained classifiers to predict the output(sentiment_class)\n",
    "#compare the prediction with the labels of the train data\n",
    "#compute accuracy of classifier\n",
    "predictions_mnb1 = mnb_classifier1.predict(transformed_test_cv)\n",
    "mnb1_f1 = 100*metrics.f1_score(y_test, predictions_mnb1, average='micro')\n",
    "print(\"f1_score for MNB classifier with CountVectorizer %r\" %(mnb1_f1) )\n",
    "\n",
    "#repeat for tfidfv\n",
    "predictions_mnb2 = mnb_classifier2.predict(transformed_test_tfidf)\n",
    "mnb2_f1 = 100*metrics.f1_score(y_test, predictions_mnb2, average='micro')                     \n",
    "print(\"f1_score for MNB classifier with Tf-Idf Vectorizer %r\" %(mnb2_f1) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for KNN classifier with CountVectorizer 59.904371584699454\n",
      "f1_score for KNN classifier with Tf-Idf Vectorizer 31.762295081967213\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest Neighbour Classifier:\n",
    "n = 2 # test value\n",
    "knn_classifier1 = KNeighborsClassifier(n_neighbors= n, weights='distance')\n",
    "knn_classifier1.fit(transformed_train_cv, y_train)\n",
    "\n",
    "predictions_knn1 = knn_classifier1.predict(transformed_test_cv)\n",
    "knn1_f1 = 100*metrics.f1_score(y_test, predictions_knn1, average='micro')\n",
    "print(\"f1_score for KNN classifier with CountVectorizer %r\" %(knn1_f1) )\n",
    "\n",
    "knn_classifier2 = KNeighborsClassifier(n_neighbors= n, weights='distance')\n",
    "knn_classifier2.fit(transformed_train_tfidf, y_train)\n",
    "\n",
    "predictions_knn2 = knn_classifier2.predict(transformed_test_tfidf)\n",
    "knn2_f1 = 100*metrics.f1_score(y_test, predictions_knn2, average='micro')\n",
    "print(\"f1_score for KNN classifier with Tf-Idf Vectorizer %r\" %(knn2_f1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for RF classifier with CountVectorizer 74.59016393442623\n",
      "f1_score for RF classifier with Tf-Idf Vectorizer 73.70218579234972\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "rf_classifier1 = RandomForestClassifier(random_state = 0)\n",
    "rf_classifier1.fit(transformed_train_cv, y_train)\n",
    "\n",
    "predictions_rf1 = rf_classifier1.predict(transformed_test_cv)\n",
    "rf1_f1 = 100*metrics.f1_score(y_test,predictions_rf1, average='micro')\n",
    "print(\"f1_score for RF classifier with CountVectorizer %r\" %(rf1_f1))\n",
    "\n",
    "rf_classifier2 = RandomForestClassifier(random_state = 0)\n",
    "rf_classifier2.fit(transformed_train_tfidf, y_train)\n",
    "\n",
    "predictions_rf2 = rf_classifier2.predict(transformed_test_tfidf)\n",
    "rf2_f1 = 100*metrics.f1_score(y_test,predictions_rf2, average='micro')\n",
    "print(\"f1_score for RF classifier with Tf-Idf Vectorizer %r\" %(rf2_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered Dataframe (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered data set\n",
    "vectorizer = tidv = TfidfVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "df_filtered = filter_dataframe(df, vectorizer)\n",
    "\n",
    "x_filtered = df_filtered['text']\n",
    "y_filtered = df_filtered['airline_sentiment'] \n",
    "x_train_filtered, x_test_filtered, y_train_filtered, y_test_filtered = df_split(\n",
    "    x_filtered, y_filtered, train_size=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11172, 7390)\n",
      "(11172, 7390)\n"
     ]
    }
   ],
   "source": [
    "cv_filtered = CountVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "transformed_train_cv_filtered = cv_filtered.fit_transform(x_train_filtered)\n",
    "print(transformed_train_cv_filtered.shape)\n",
    "\n",
    "tidv_filtered = TfidfVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "transformed_train_tfidf_filtered = tidv_filtered.fit_transform(x_train_filtered)\n",
    "print(transformed_train_tfidf_filtered.shape)\n",
    "\n",
    "transformed_test_cv_filtered = cv_filtered.transform(x_test_filtered)\n",
    "transformed_test_tfidf_filtered = tidv_filtered.transform(x_test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnb_classifier1 = MultinomialNB(alpha=1)\n",
    "fmnb_classifier1.fit(transformed_train_cv_filtered, y_train_filtered)\n",
    "\n",
    "fmnb_classifier2 = MultinomialNB(alpha=1)\n",
    "fmnb_classifier2.fit(transformed_train_tfidf_filtered, y_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for MNB classifier with CountVectorizer 74.58840372226197\n",
      "f1_score for MNB classifier with Tf-Idf Vectorizer 66.96492483894059\n"
     ]
    }
   ],
   "source": [
    "fpredictions_mnb1 = fmnb_classifier1.predict(transformed_test_cv_filtered)\n",
    "fmnb1_f1 = 100*metrics.f1_score(y_test_filtered, fpredictions_mnb1, average='micro')\n",
    "print(\"f1_score for MNB classifier with CountVectorizer %r\" %(fmnb1_f1))\n",
    "\n",
    "fpredictions_mnb2 = fmnb_classifier2.predict(transformed_test_tfidf_filtered)\n",
    "fmnb2_f1 = 100*metrics.f1_score(y_test_filtered, fpredictions_mnb2, average='micro')\n",
    "print(\"f1_score for MNB classifier with Tf-Idf Vectorizer %r\" %(fmnb2_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for KNN classifier with CountVectorizer 61.27415891195419\n",
      "f1_score for KNN classifier with Tf-Idf Vectorizer 29.133858267716533\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest Neighbour Classifier:\n",
    "n = 2 \n",
    "fknn_classifier1 = KNeighborsClassifier(n_neighbors=n, weights='distance')\n",
    "fknn_classifier1.fit(transformed_train_cv_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_knn1 = fknn_classifier1.predict(transformed_test_cv_filtered)\n",
    "fknn1_f1 = 100*metrics.f1_score(y_test_filtered, fpredictions_knn1, average='micro')\n",
    "print(\"f1_score for KNN classifier with CountVectorizer %r\"%(fknn1_f1))\n",
    "\n",
    "fknn_classifier2 = KNeighborsClassifier(n_neighbors= n, weights='distance')\n",
    "fknn_classifier2.fit(transformed_train_tfidf_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_knn2 = fknn_classifier2.predict(transformed_test_tfidf_filtered)\n",
    "fknn2_f1 = 100*metrics.f1_score(y_test_filtered, fpredictions_knn2, average='micro')\n",
    "print(\"f1_score for KNN classifier with Tf-Idf Vectorizer %r\"%(fknn2_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for RF classifier with Count Vectorizer 73.87258410880459\n",
      "f1_score for RF classifier with Tf-Idf Vectorizer 24.874731567644954\n"
     ]
    }
   ],
   "source": [
    "#Random Forest classifier\n",
    "frf_classifier1 = RandomForestClassifier(random_state = 0)\n",
    "frf_classifier1.fit(transformed_train_cv_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_rf1 = frf_classifier1.predict(transformed_test_cv_filtered)\n",
    "frf1_f1 = 100*metrics.f1_score(y_test_filtered,fpredictions_rf1, average='micro')\n",
    "print(\"f1_score for RF classifier with Count Vectorizer %r\"%(frf1_f1))\n",
    "\n",
    "frf_classifier2 = RandomForestClassifier(random_state = 0)\n",
    "frf_classifier2.fit(transformed_train_cv_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_rf2 = frf_classifier2.predict(transformed_test_tfidf_filtered)\n",
    "frf2_f1 = 100*metrics.f1_score(y_test_filtered,fpredictions_rf2, average='micro')\n",
    "print(\"f1_score for RF classifier with Tf-Idf Vectorizer %r\"%(frf2_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores' values differ post-filteration, but since we shuffle the data set, you may not notice the difference if you re-run.\n",
    "The scores are usually LESS after filteration than before filteration.\n",
    "\n",
    "Also, when attempting the usage of different Vectorizers (Count VS. TF-IDF) the scores were usually completely different and the TF-IDF usually had much worse scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for MNB classifier(CV): \n",
      "Before Filteration: 75.1707650273224 \n",
      "AfterFilteration: 74.58840372226197\n",
      "\n",
      "f1_score for MNB classifier(TFIDF): \n",
      "Before Filteration: 68.4084699453552 \n",
      "AfterFilteration: 66.96492483894059\n",
      "\n",
      "f1_score for KNN classifier(CV): \n",
      "Before Filteration: 59.904371584699454 \n",
      "AfterFilteration: 61.27415891195419\n",
      "\n",
      "f1_score for KNN classifier(TFIDF): \n",
      "Before Filteration: 31.762295081967213 \n",
      "AfterFilteration: 29.133858267716533\n",
      "\n",
      "f1_score for RF classifier(CV): \n",
      "Before Filteration: 74.59016393442623 \n",
      "AfterFilteration: 73.87258410880459\n",
      "\n",
      "f1_score for RF classifier(TFIDF): \n",
      "Before Filteration: 73.70218579234972 \n",
      "AfterFilteration: 24.874731567644954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score for MNB classifier(CV): \\nBefore Filteration: %r \\nAfterFilteration: %r\\n\" %(mnb1_f1, fmnb1_f1) )\n",
    "print(\"f1_score for MNB classifier(TFIDF): \\nBefore Filteration: %r \\nAfterFilteration: %r\\n\" %(mnb2_f1, fmnb2_f1) )\n",
    "\n",
    "print(\"f1_score for KNN classifier(CV): \\nBefore Filteration: %r \\nAfterFilteration: %r\\n\" %(knn1_f1, fknn1_f1) )\n",
    "print(\"f1_score for KNN classifier(TFIDF): \\nBefore Filteration: %r \\nAfterFilteration: %r\\n\" %(knn2_f1, fknn2_f1) )\n",
    "\n",
    "print(\"f1_score for RF classifier(CV): \\nBefore Filteration: %r \\nAfterFilteration: %r\\n\" %(rf1_f1, frf1_f1) )\n",
    "print(\"f1_score for RF classifier(TFIDF): \\nBefore Filteration: %r \\nAfterFilteration: %r\\n\" %(rf2_f1, frf2_f1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads ds \n",
    "df_sent140 = pd.read_csv('./sentiment140.csv', encoding='latin-1',  \n",
    "                         names = [\"polarity\", \"tweet_id\", \"tweet_time\", \"query\",\"tweep_handle\", \"text\"])\n",
    "#convert the int classes to strings \n",
    "df_sent140['polarity'] = df_sent140['polarity'].astype(str)\n",
    "#show the df\n",
    "df_sent140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of classes: only two(this dataset does not include the neutral class)\n",
    "set(np.array(df_sent140.polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data set \n",
    "x = np.array(df_sent140['text'][:20000])\n",
    "y = np.array(df_sent140['polarity'][:20000])\n",
    "x_train, x_test, y_train, y_test = df_split(x, y, train_size = 0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#either the data set is corrupted: there are similar tweets with different labels \n",
    "#(you can discover this in MultiNomialClassifier) \n",
    "#OR the neutral class has all its entries once labeled as +ve and once as -ve \n",
    "len(set(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, we could remove all the cosine-similar tweets from the dataset, and avoid all corrupted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, we  used the manual data set with the 3 classes (neutral- pos- neg) but the set size was relatively very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads ds \n",
    "df_sent140 = pd.read_csv('./manualds.csv', encoding='latin-1',  \n",
    "    names = [\"polarity\", \"tweet_id\", \"tweet_time\", \"query\",\"tweep_handle\", \"text\"])\n",
    "df_sent140['polarity'] = df_sent140['polarity'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(np.array(df_sent140.polarity)))) # 3 classes\n",
    "\n",
    "x = np.array(df_sent140['text'])\n",
    "print(len(set(x)))  \n",
    "y = np.array(df_sent140['polarity'])\n",
    "x_train, x_test, y_train, y_test = df_split(x, y, train_size = 0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer; constructs a document-word count matrix\n",
    "cv = CountVectorizer(tokenizer= lambda t: list( preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "transformed_train_cv = cv.fit_transform(x_train)\n",
    "print(transformed_train_cv.shape)\n",
    "\n",
    "#Tf-Idf Vectorizer; constructs a tf-idf matrix\n",
    "tidv = TfidfVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii', binary=True)\n",
    "transformed_train_tfidf = tidv.fit_transform(x_train)\n",
    "print(transformed_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the test inputs using cv and tfidf(no fitting is done on test)\n",
    "transformed_test_cv = cv.transform(x_test)\n",
    "transformed_test_tfidf = tidv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-nomial NB classifier:\n",
    "#using count vectorizer: feed the classifier train inputs, and labels\n",
    "mnb_classifier1 = MultinomialNB()\n",
    "mnb_classifier1.fit(transformed_train_cv, y_train)\n",
    "\n",
    "#using tfidf vectorizer\n",
    "mnb_classifier2 = MultinomialNB()\n",
    "mnb_classifier2.fit(transformed_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the trained classifiers to predict the output(sentiment_class)\n",
    "#compare the prediction with the labels of the train data\n",
    "#compute accuracy of classifier\n",
    "predictions_mnb1 = mnb_classifier1.predict(transformed_test_cv)\n",
    "mnb1_f1 = 100*metrics.f1_score(y_test, predictions_mnb1, average='micro')\n",
    "print(\"f1_score for MNB classifier with CountVectorizer %r\" %(mnb1_f1) )\n",
    "\n",
    "#repeat for tfidfv\n",
    "predictions_mnb2 = mnb_classifier2.predict(transformed_test_tfidf)\n",
    "mnb2_f1 = 100*metrics.f1_score(y_test, predictions_mnb2, average='micro')                     \n",
    "print(\"f1_score for MNB classifier with Tf-Idf Vectorizer %r\" %(mnb2_f1) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Nearest Neighbour Classifier:\n",
    "n = 2 # test value\n",
    "knn_classifier1 = KNeighborsClassifier(n_neighbors= n, weights='distance')\n",
    "knn_classifier1.fit(transformed_train_cv, y_train)\n",
    "\n",
    "predictions_knn1 = knn_classifier1.predict(transformed_test_cv)\n",
    "knn1_f1 = 100*metrics.f1_score(y_test, predictions_knn1, average='micro')\n",
    "print(\"f1_score for KNN classifier with CountVectorizer %r\" %(knn1_f1) )\n",
    "\n",
    "knn_classifier2 = KNeighborsClassifier(n_neighbors= n, weights='distance')\n",
    "knn_classifier2.fit(transformed_train_tfidf, y_train)\n",
    "\n",
    "predictions_knn2 = knn_classifier2.predict(transformed_test_tfidf)\n",
    "knn2_f1 = 100*metrics.f1_score(y_test, predictions_knn2, average='micro')\n",
    "print(\"f1_score for KNN classifier with Tf-Idf Vectorizer %r\" %(knn2_f1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "rf_classifier1 = RandomForestClassifier(random_state = 0)\n",
    "rf_classifier1.fit(transformed_train_cv, y_train)\n",
    "\n",
    "predictions_rf1 = rf_classifier1.predict(transformed_test_cv)\n",
    "rf1_f1 = 100*metrics.f1_score(y_test,predictions_rf1, average='micro')\n",
    "print(\"f1_score for RF classifier with CountVectorizer %r\" %(rf1_f1))\n",
    "\n",
    "rf_classifier2 = RandomForestClassifier(random_state = 0)\n",
    "rf_classifier2.fit(transformed_train_tfidf, y_train)\n",
    "\n",
    "predictions_rf2 = rf_classifier2.predict(transformed_test_tfidf)\n",
    "rf2_f1 = 100*metrics.f1_score(y_test,predictions_rf2, average='micro')\n",
    "print(\"f1_score for RF classifier with Tf-Idf Vectorizer %r\" %(rf2_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered data set\n",
    "vectorizer = tidv = TfidfVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "df_filtered = filter_dataframe(df_sent140, vectorizer)\n",
    "\n",
    "x_filtered = df_filtered['text'][:20000]\n",
    "y_filtered = df_filtered['polarity'][:20000] \n",
    "x_train_filtered, x_test_filtered, y_train_filtered, y_test_filtered = df_split(\n",
    "    x_filtered, y_filtered, train_size=0.8, shuffle=True)\n",
    "\n",
    "cv_filtered = CountVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "transformed_train_cv_filtered = cv_filtered.fit_transform(x_train_filtered)\n",
    "print(transformed_train_cv_filtered.shape)\n",
    "\n",
    "tidv_filtered = TfidfVectorizer(tokenizer= lambda t: list(preprocess_and_tokenize(t)), strip_accents='ascii')\n",
    "transformed_train_tfidf_filtered = tidv_filtered.fit_transform(x_train_filtered)\n",
    "print(transformed_train_tfidf_filtered.shape)\n",
    "\n",
    "transformed_test_cv_filtered = cv_filtered.transform(x_test_filtered)\n",
    "transformed_test_tfidf_filtered = tidv_filtered.transform(x_test_filtered)\n",
    "\n",
    "fmnb_classifier1 = MultinomialNB(alpha=1)\n",
    "fmnb_classifier1.fit(transformed_train_cv_filtered, y_train_filtered)\n",
    "\n",
    "fmnb_classifier2 = MultinomialNB(alpha=1)\n",
    "fmnb_classifier2.fit(transformed_train_tfidf_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_mnb1 = fmnb_classifier1.predict(transformed_test_cv_filtered)\n",
    "fmnb1_f1 = 100*metrics.f1_score(y_test_filtered, fpredictions_mnb1, average='micro')\n",
    "print(\"f1_score for MNB classifier with CountVectorizer %r\" %(fmnb1_f1))\n",
    "\n",
    "fpredictions_mnb2 = fmnb_classifier2.predict(transformed_test_tfidf_filtered)\n",
    "fmnb2_f1 = 100*metrics.f1_score(y_test_filtered, fpredictions_mnb2, average='micro')\n",
    "print(\"f1_score for MNB classifier with Tf-Idf Vectorizer %r\" %(fmnb2_f1))\n",
    "\n",
    "#K-Nearest Neighbour Classifier:\n",
    "n = 2 \n",
    "fknn_classifier1 = KNeighborsClassifier(n_neighbors=n, weights='distance')\n",
    "fknn_classifier1.fit(transformed_train_cv_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_knn1 = fknn_classifier1.predict(transformed_test_cv_filtered)\n",
    "fknn1_f1 = 100*metrics.f1_score(y_test_filtered, fpredictions_knn1, average='micro')\n",
    "print(\"f1_score for KNN classifier with CountVectorizer %r\"%(fknn1_f1))\n",
    "\n",
    "fknn_classifier2 = KNeighborsClassifier(n_neighbors= n, weights='distance')\n",
    "fknn_classifier2.fit(transformed_train_tfidf_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_knn2 = fknn_classifier2.predict(transformed_test_tfidf_filtered)\n",
    "fknn2_f1 = 100*metrics.f1_score(y_test_filtered, fpredictions_knn2, average='micro')\n",
    "print(\"f1_score for KNN classifier with Tf-Idf Vectorizer %r\"%(fknn2_f1))\n",
    "\n",
    "frf_classifier1 = RandomForestClassifier(random_state = 0)\n",
    "frf_classifier1.fit(transformed_train_cv_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_rf1 = frf_classifier1.predict(transformed_test_cv_filtered)\n",
    "frf1_f1 = 100*metrics.f1_score(y_test_filtered,fpredictions_rf1, average='micro')\n",
    "print(\"f1_score for RF classifier with Count Vectorizer %r\"%(frf1_f1))\n",
    "\n",
    "frf_classifier2 = RandomForestClassifier(random_state = 0)\n",
    "frf_classifier2.fit(transformed_train_cv_filtered, y_train_filtered)\n",
    "\n",
    "fpredictions_rf2 = frf_classifier2.predict(transformed_test_tfidf_filtered)\n",
    "frf2_f1 = 100*metrics.f1_score(y_test_filtered,fpredictions_rf2, average='micro')\n",
    "print(\"f1_score for RF classifier with Count Vectorizer %r\"%(frf2_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
